{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d3a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "import joblib\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b570f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\")\n",
    "\n",
    "# Create a corpus from the dataset\n",
    "corpus = {}\n",
    "\n",
    "\n",
    "for doc in dataset.docs_iter():\n",
    "        corpus[doc.doc_id]= doc.title+\" \" + doc.summary+\" \" +doc.detailed_description+ \" \"+doc.eligibility\n",
    "\n",
    "\n",
    "        \n",
    "documents = list(corpus.values())\n",
    "\n",
    "def custom_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"Tokenizes and lowercases the text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Converts POS tag to a format that WordNetLemmatizer can understand.\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def remove_markers(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes specific markers from tokens.\"\"\"\n",
    "    return [re.sub(r'\\u00AE', '', token) for token in tokens]\n",
    "\n",
    "def remove_punctuation(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes punctuation from tokens.\"\"\"\n",
    "    return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "def replace_under_score_with_space(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Replaces underscores with spaces in tokens.\"\"\"\n",
    "    return [re.sub(r'_', ' ', token) for token in tokens]\n",
    "\n",
    "def remove_apostrophe(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes apostrophes from tokens.\"\"\"\n",
    "    return [token.replace(\"'\", \" \") for token in tokens]\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocesses the input text by tokenizing, removing punctuation, stopwords, and then stemming and lemmatizing.\"\"\"\n",
    "    # Convert text to lowercase and tokenize\n",
    "    words = custom_tokenizer(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "    # Further token cleaning\n",
    "    words = remove_markers(words)\n",
    "    words = replace_under_score_with_space(words)\n",
    "    words = remove_apostrophe(words)\n",
    "    \n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177930e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess documents\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec([doc.split() for doc in processed_documents], vector_size=100, sg=1, epochs=37)\n",
    "# Save the Word2Vec model\n",
    "word2vec_model.save(\"word2vec_model.kv\")\n",
    "# Load the Word2Vec model\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.kv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28290fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def vectorize_documents(documents: List[str]) -> List[np.ndarray]:\n",
    "    document_vectors = []\n",
    "    for document in documents:\n",
    "        vectors = [word2vec_model.wv[token] for token in document.split() if token in word2vec_model.wv]\n",
    "        if vectors:\n",
    "            document_vectors.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            document_vectors.append(np.zeros(100))\n",
    "    return document_vectors\n",
    "\n",
    "# Compute document vectors\n",
    "doc_vectors = vectorize_documents(processed_documents)\n",
    "print(doc_vectors)\n",
    "# Save and load functions for TF-IDF data\n",
    "def save_file(file_location: str, content):\n",
    "    with open(file_location, 'wb') as file:\n",
    "        pickle.dump(content, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_file(file_location: str):\n",
    "    with open(file_location, 'rb') as file:\n",
    "        loaded_file = pickle.load(file)\n",
    "    return loaded_file\n",
    "save_file(\"doc_vectors.pkl\",doc_vectors)\n",
    "doc_vectors = load_file(\"doc_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991f822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance_scores(query_text: str) -> List[str]:\n",
    "    \"\"\"Compute relevance scores between a query vector and all document vectors.\"\"\"\n",
    "    query_tokens = preprocess_text(query_text)\n",
    "    query_vec = vectorize_documents([query_tokens])[0].reshape(1, -1)\n",
    "    similarities = cosine_similarity(doc_vectors, query_vec)\n",
    "    top_10_indices = similarities.argsort(axis=0)[-10:][::-1].flatten()\n",
    "    return [list(corpus.keys())[index] for index in top_10_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd4b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query id: 1, Recall@10: 0.011834319526627219\n",
      "query id: 1, Precision@10: 0.2\n",
      "query id: 2, Recall@10: 0.025925925925925925\n",
      "query id: 2, Precision@10: 0.7\n",
      "query id: 3, Recall@10: 0.011904761904761904\n",
      "query id: 3, Precision@10: 0.1\n",
      "query id: 4, Recall@10: 0.0\n",
      "query id: 4, Precision@10: 0.0\n",
      "query id: 5, Recall@10: 0.01990049751243781\n",
      "query id: 5, Precision@10: 0.4\n",
      "query id: 6, Recall@10: 0.01904761904761905\n",
      "query id: 6, Precision@10: 0.4\n",
      "query id: 7, Recall@10: 0.031055900621118012\n",
      "query id: 7, Precision@10: 0.5\n",
      "query id: 8, Recall@10: 0.032679738562091505\n",
      "query id: 8, Precision@10: 0.5\n",
      "query id: 9, Recall@10: 0.014018691588785047\n",
      "query id: 9, Precision@10: 0.3\n",
      "query id: 10, Recall@10: 0.0\n",
      "query id: 10, Precision@10: 0.0\n",
      "query id: 11, Recall@10: 0.007936507936507936\n",
      "query id: 11, Precision@10: 0.1\n",
      "query id: 12, Recall@10: 0.020134228187919462\n",
      "query id: 12, Precision@10: 0.3\n",
      "query id: 13, Recall@10: 0.0\n",
      "query id: 13, Precision@10: 0.0\n",
      "query id: 14, Recall@10: 0.01020408163265306\n",
      "query id: 14, Precision@10: 0.3\n",
      "query id: 15, Recall@10: 0.0038461538461538464\n",
      "query id: 15, Precision@10: 0.1\n",
      "query id: 16, Recall@10: 0.036231884057971016\n",
      "query id: 16, Precision@10: 0.5\n",
      "query id: 17, Recall@10: 0.02390438247011952\n",
      "query id: 17, Precision@10: 0.6\n",
      "query id: 18, Recall@10: 0.014563106796116505\n",
      "query id: 18, Precision@10: 0.3\n",
      "query id: 19, Recall@10: 0.032679738562091505\n",
      "query id: 19, Precision@10: 0.5\n",
      "query id: 20, Recall@10: 0.014285714285714285\n",
      "query id: 20, Precision@10: 0.1\n",
      "query id: 21, Recall@10: 0.0\n",
      "query id: 21, Precision@10: 0.0\n",
      "query id: 22, Recall@10: 0.029197080291970802\n",
      "query id: 22, Precision@10: 0.4\n",
      "query id: 23, Recall@10: 0.018726591760299626\n",
      "query id: 23, Precision@10: 0.5\n",
      "query id: 24, Recall@10: 0.0\n",
      "query id: 24, Precision@10: 0.0\n",
      "query id: 25, Recall@10: 0.029411764705882353\n",
      "query id: 25, Precision@10: 0.3\n",
      "query id: 26, Recall@10: 0.01875\n",
      "query id: 26, Precision@10: 0.3\n",
      "query id: 27, Recall@10: 0.012875536480686695\n",
      "query id: 27, Precision@10: 0.3\n",
      "query id: 28, Recall@10: 0.0\n",
      "query id: 28, Precision@10: 0.0\n",
      "query id: 29, Recall@10: 0.015503875968992248\n",
      "query id: 29, Precision@10: 0.4\n",
      "query id: 30, Recall@10: 0.03333333333333333\n",
      "query id: 30, Precision@10: 0.5\n",
      "query id: 31, Recall@10: 0.011627906976744186\n",
      "query id: 31, Precision@10: 0.2\n",
      "query id: 32, Recall@10: 0.0\n",
      "query id: 32, Precision@10: 0.0\n",
      "query id: 33, Recall@10: 0.0\n",
      "query id: 33, Precision@10: 0.0\n",
      "query id: 34, Recall@10: 0.013333333333333334\n",
      "query id: 34, Precision@10: 0.2\n",
      "query id: 35, Recall@10: 0.022988505747126436\n",
      "query id: 35, Precision@10: 0.4\n",
      "query id: 36, Recall@10: 0.012944983818770227\n",
      "query id: 36, Precision@10: 0.4\n",
      "query id: 37, Recall@10: 0.004032258064516129\n",
      "query id: 37, Precision@10: 0.1\n",
      "query id: 38, Recall@10: 0.0\n",
      "query id: 38, Precision@10: 0.0\n",
      "query id: 39, Recall@10: 0.0\n",
      "query id: 39, Precision@10: 0.0\n",
      "query id: 40, Recall@10: 0.07692307692307693\n",
      "query id: 40, Precision@10: 0.1\n",
      "query id: 41, Recall@10: 0.006493506493506494\n",
      "query id: 41, Precision@10: 0.2\n",
      "query id: 42, Recall@10: 0.0\n",
      "query id: 42, Precision@10: 0.0\n",
      "query id: 43, Recall@10: 0.01935483870967742\n",
      "query id: 43, Precision@10: 0.3\n",
      "query id: 44, Recall@10: 0.0\n",
      "query id: 44, Precision@10: 0.0\n",
      "query id: 45, Recall@10: 0.010752688172043012\n",
      "query id: 45, Precision@10: 0.2\n",
      "query id: 46, Recall@10: 0.0\n",
      "query id: 46, Precision@10: 0.0\n",
      "query id: 47, Recall@10: 0.007518796992481203\n",
      "query id: 47, Precision@10: 0.2\n",
      "query id: 48, Recall@10: 0.030303030303030304\n",
      "query id: 48, Precision@10: 0.2\n",
      "query id: 49, Recall@10: 0.03896103896103896\n",
      "query id: 49, Precision@10: 0.3\n",
      "query id: 50, Recall@10: 0.13043478260869565\n",
      "query id: 50, Precision@10: 0.3\n",
      "query id: 51, Recall@10: 0.04225352112676056\n",
      "query id: 51, Precision@10: 0.3\n",
      "query id: 52, Recall@10: 0.04411764705882353\n",
      "query id: 52, Precision@10: 0.3\n",
      "query id: 53, Recall@10: 0.0\n",
      "query id: 53, Precision@10: 0.0\n",
      "query id: 54, Recall@10: 0.0\n",
      "query id: 54, Precision@10: 0.0\n",
      "query id: 55, Recall@10: 0.0\n",
      "query id: 55, Precision@10: 0.0\n",
      "query id: 56, Recall@10: 0.08139534883720931\n",
      "query id: 56, Precision@10: 0.7\n",
      "query id: 57, Recall@10: 0.006172839506172839\n",
      "query id: 57, Precision@10: 0.1\n",
      "query id: 58, Recall@10: 0.022058823529411766\n",
      "query id: 58, Precision@10: 0.3\n",
      "query id: 59, Recall@10: 0.011396011396011397\n",
      "query id: 59, Precision@10: 0.4\n",
      "query id: 60, Recall@10: 0.010050251256281407\n",
      "query id: 60, Precision@10: 0.2\n",
      "query id: 61, Recall@10: 0.0\n",
      "query id: 61, Precision@10: 0.0\n",
      "query id: 62, Recall@10: 0.0036363636363636364\n",
      "query id: 62, Precision@10: 0.1\n",
      "query id: 63, Recall@10: 0.045454545454545456\n",
      "query id: 63, Precision@10: 0.2\n",
      "query id: 64, Recall@10: 0.009852216748768473\n",
      "query id: 64, Precision@10: 0.2\n",
      "query id: 65, Recall@10: 0.008849557522123894\n",
      "query id: 65, Precision@10: 0.1\n",
      "query id: 66, Recall@10: 0.020833333333333332\n",
      "query id: 66, Precision@10: 0.1\n",
      "query id: 67, Recall@10: 0.012738853503184714\n",
      "query id: 67, Precision@10: 0.2\n",
      "query id: 68, Recall@10: 0.024390243902439025\n",
      "query id: 68, Precision@10: 0.4\n",
      "query id: 69, Recall@10: 0.0\n",
      "query id: 69, Precision@10: 0.0\n",
      "query id: 70, Recall@10: 0.004329004329004329\n",
      "query id: 70, Precision@10: 0.1\n",
      "query id: 71, Recall@10: 0.016304347826086956\n",
      "query id: 71, Precision@10: 0.3\n",
      "query id: 72, Recall@10: 0.01098901098901099\n",
      "query id: 72, Precision@10: 0.1\n",
      "query id: 73, Recall@10: 0.0\n",
      "query id: 73, Precision@10: 0.0\n",
      "query id: 74, Recall@10: 0.07692307692307693\n",
      "query id: 74, Precision@10: 0.1\n",
      "query id: 75, Recall@10: 0.008797653958944282\n",
      "query id: 75, Precision@10: 0.3\n"
     ]
    }
   ],
   "source": [
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs = []\n",
    "    retrieved_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = compute_relevance_scores(query[1])\n",
    "            break\n",
    "\n",
    "    truncated_retrieved_docs = retrieved_docs[:len(relevant_docs)]\n",
    "    y_true = [1 if doc in relevant_docs else 0 for doc in retrieved_docs]\n",
    "    true_positives = sum(1 for doc in truncated_retrieved_docs if doc in relevant_docs)\n",
    "    recall_at_10 = true_positives / len(relevant_docs)\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"query id: {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"query id: {query_id}, Precision@10: {precision_at_10}\")\n",
    "\n",
    "    return recall_at_10\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "for query_id in queries_ids.keys():\n",
    "    calculate_recall_precision(query_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf90bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = [qrel[1] for qrel in dataset.qrels_iter() if qrel[0] == query_id]\n",
    "    relevant_docs = []\n",
    "\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            relevant_docs = compute_relevance_scores(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = sum(1 for j in range(i) if j < len(relevant_docs) and relevant_docs[j] in relevant_docs)\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(relevant_docs) and relevant_docs[i - 1] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i - 1 < len(relevant_docs) and relevant_docs[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "map_sum = sum(calculate_MAP(query_id) for query_id in queries_ids.keys())\n",
    "print(map_sum / dataset.queries_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e28e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank : 0.4187407407407407\n"
     ]
    }
   ],
   "source": [
    "def calculate_MRR(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel.query_id == query_id and qrel.relevance > 0:\n",
    "            relevant_docs.append(qrel.doc_id)\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query.query_id == query_id:\n",
    "            retrieved_docs = compute_relevance_scores(query.text)\n",
    "            break\n",
    "\n",
    "    for i, result in enumerate(retrieved_docs):\n",
    "        if result in relevant_docs:\n",
    "            return 1 / (i + 1)\n",
    "\n",
    "    return 0\n",
    "\n",
    "queries_ids = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    queries_ids.update({qrel.query_id: ''})\n",
    "\n",
    "mrr_sum = 0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    mrr_sum += calculate_MRR(query_id)\n",
    "\n",
    "print(f\"Mean Reciprocal Rank : {mrr_sum / len(queries_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f634f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5324335159989921\n"
     ]
    }
   ],
   "source": [
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = [qrel[1] for qrel in dataset.qrels_iter() if qrel[0] == query_id]\n",
    "    ordered_results = []\n",
    "\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            ordered_results = compute_relevance_scores(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = sum(1 for j in range(i) if j < len(ordered_results) and ordered_results[j] in relevant_docs)\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(ordered_results) and ordered_results[i - 1] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i - 1 < len(ordered_results) and ordered_results[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "map_sum = sum(calculate_MAP(query_id) for query_id in queries_ids.keys())\n",
    "print(map_sum / dataset.queries_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac6d25-7ffa-4e9d-a309-45a9ed4d14a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
