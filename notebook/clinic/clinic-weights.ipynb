{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2135581-b4d7-456f-9cf1-00f049d15e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\")\n",
    "doc_ids = []\n",
    "titles = []\n",
    "summaries = []\n",
    "detailed_descriptions = []\n",
    "eligibilities = []\n",
    "counter = 0\n",
    "for doc in dataset.docs_iter():\n",
    "    if counter < 5000:\n",
    "        doc_ids.append(doc.doc_id)\n",
    "        titles.append(doc.title)\n",
    "        summaries.append(doc.summary)\n",
    "        detailed_descriptions.append(doc.detailed_description)\n",
    "        eligibilities.append(doc.eligibility)\n",
    "        counter += 1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# Define the corpus as a dictionary with doc_id as the key and separate text fields\n",
    "corpus = {\n",
    "    doc_ids[i]: {\n",
    "        'title': titles[i],\n",
    "        'summary': summaries[i],\n",
    "        'detailed_description': detailed_descriptions[i],\n",
    "        'eligibility': eligibilities[i]\n",
    "    } for i in range(len(doc_ids))\n",
    "}\n",
    "\n",
    "\n",
    "def custom_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"Tokenizes and lowercases the text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    # \"\"\"Converts POS tag to a format that WordNetLemmatizer can understand.\"\"\"\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def remove_markers(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes specific markers from tokens.\"\"\"\n",
    "    return [re.sub(r'\\u00AE', '', token) for token in tokens]\n",
    "\n",
    "def remove_punctuation(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes punctuation from tokens.\"\"\"\n",
    "    return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "def replace_under_score_with_space(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Replaces underscores with spaces in tokens.\"\"\"\n",
    "    return [re.sub(r'_', ' ', token) for token in tokens]\n",
    "\n",
    "def remove_apostrophe(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes apostrophes from tokens.\"\"\"\n",
    "    return [token.replace(\"'\", \" \") for token in tokens]\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocesses the input text by tokenizing, removing punctuation, stopwords, and then stemming and lemmatizing.\"\"\"\n",
    "    # Convert text to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Correct spelling\n",
    "    # words = correct_sentence_spelling(words)\n",
    "    \n",
    "    # Further token cleaning\n",
    "    words = remove_markers(words)\n",
    "    words = replace_under_score_with_space(words)\n",
    "    words = remove_apostrophe(words)\n",
    "    \n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_titles = [preprocess_text(title) for title in titles]\n",
    "preprocessed_summaries = [preprocess_text(summary) for summary in summaries]\n",
    "preprocessed_detailed_descriptions = [preprocess_text(description) for description in detailed_descriptions]\n",
    "preprocessed_eligibilities = [preprocess_text(eligibility) for eligibility in eligibilities]\n",
    "# Save preprocessed data to a file\n",
    "preprocessed_corpus = {\n",
    "    'titles': preprocessed_titles,\n",
    "    'summaries': preprocessed_summaries,\n",
    "    'detailed_descriptions': preprocessed_detailed_descriptions,\n",
    "    'eligibilities': preprocessed_eligibilities\n",
    "}\n",
    "\n",
    "with open('preprocessed_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_corpus, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load preprocessed data from the file\n",
    "with open('preprocessed_corpus.pkl', 'rb') as f:\n",
    "    preprocessed_corpus = pickle.load(f)\n",
    "\n",
    "\n",
    "# Combine all preprocessed texts into a single list for vectorizer fitting\n",
    "all_texts = (\n",
    "    preprocessed_corpus['titles'] +\n",
    "    preprocessed_corpus['summaries'] +\n",
    "    preprocessed_corpus['detailed_descriptions'] +\n",
    "    preprocessed_corpus['eligibilities']\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the combined corpus\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "# Transform each column separately using the fitted vectorizer\n",
    "tfidf_titles = vectorizer.transform(preprocessed_corpus['titles'])\n",
    "tfidf_summaries = vectorizer.transform(preprocessed_corpus['summaries'])\n",
    "tfidf_detailed_descriptions = vectorizer.transform(preprocessed_corpus['detailed_descriptions'])\n",
    "tfidf_eligibilities = vectorizer.transform(preprocessed_corpus['eligibilities'])\n",
    "\n",
    "# Define weights for each column\n",
    "weights = {\n",
    "    'title': 5,\n",
    "    'summary': 2,\n",
    "    'detailed_description': 1,\n",
    "    'eligibility': 4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Combine weighted TF-IDF vectors with normalization\n",
    "combined_tfidf = (\n",
    "    weights['title'] * tfidf_titles +\n",
    "    weights['summary'] * tfidf_summaries +\n",
    "    weights['detailed_description'] * tfidf_detailed_descriptions +\n",
    "    weights['eligibility'] * tfidf_eligibilities\n",
    ") / (weights['title'] + weights['summary'] + weights['detailed_description'] + weights['eligibility'])\n",
    "\n",
    "tfidf_model = vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24361a89-a132-44ba-a65d-d4a655dfa904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save and load functions for TF-IDF data\n",
    "def save_file(file_location: str, content):\n",
    "    if os.path.exists(file_location):\n",
    "        os.remove(file_location)\n",
    "    with open(file_location, 'wb') as handle:\n",
    "        pickle.dump(content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_file(file_location: str):\n",
    "    with open(file_location, 'rb') as handle:\n",
    "        content = pickle.load(handle)\n",
    "    return content\n",
    "\n",
    "def save_tfidf_data(tfidf_matrix, tfidf_model):\n",
    "    save_file(\"tfidf_matrix.pickle\", tfidf_matrix)\n",
    "    save_file(\"tfidf_model.pickle\", tfidf_model)\n",
    "\n",
    "save_tfidf_data(combined_tfidf, tfidf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58b45fe4-67aa-472b-a613-149bdf9cd1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1, Recall@10: 0.05325443786982249\n",
      "Query ID: 1, Precision@10: 0.9\n",
      "Query ID: 2, Recall@10: 0.0\n",
      "Query ID: 2, Precision@10: 0.0\n",
      "Query ID: 3, Recall@10: 0.011904761904761904\n",
      "Query ID: 3, Precision@10: 0.1\n",
      "Query ID: 4, Recall@10: 0.010526315789473684\n",
      "Query ID: 4, Precision@10: 0.1\n",
      "Query ID: 5, Recall@10: 0.014925373134328358\n",
      "Query ID: 5, Precision@10: 0.3\n",
      "Query ID: 6, Recall@10: 0.0\n",
      "Query ID: 6, Precision@10: 0.0\n",
      "Query ID: 7, Recall@10: 0.006211180124223602\n",
      "Query ID: 7, Precision@10: 0.1\n",
      "Query ID: 8, Recall@10: 0.006535947712418301\n",
      "Query ID: 8, Precision@10: 0.1\n",
      "Query ID: 9, Recall@10: 0.009345794392523364\n",
      "Query ID: 9, Precision@10: 0.2\n",
      "Query ID: 10, Recall@10: 0.02127659574468085\n",
      "Query ID: 10, Precision@10: 0.1\n",
      "Query ID: 11, Recall@10: 0.015873015873015872\n",
      "Query ID: 11, Precision@10: 0.2\n",
      "Query ID: 12, Recall@10: 0.006711409395973154\n",
      "Query ID: 12, Precision@10: 0.1\n",
      "Query ID: 13, Recall@10: 0.0\n",
      "Query ID: 13, Precision@10: 0.0\n",
      "Query ID: 14, Recall@10: 0.0\n",
      "Query ID: 14, Precision@10: 0.0\n",
      "Query ID: 15, Recall@10: 0.007692307692307693\n",
      "Query ID: 15, Precision@10: 0.2\n",
      "Query ID: 16, Recall@10: 0.0\n",
      "Query ID: 16, Precision@10: 0.0\n",
      "Query ID: 17, Recall@10: 0.01593625498007968\n",
      "Query ID: 17, Precision@10: 0.4\n",
      "Query ID: 18, Recall@10: 0.0\n",
      "Query ID: 18, Precision@10: 0.0\n",
      "Query ID: 19, Recall@10: 0.013071895424836602\n",
      "Query ID: 19, Precision@10: 0.2\n",
      "Query ID: 20, Recall@10: 0.04285714285714286\n",
      "Query ID: 20, Precision@10: 0.3\n",
      "Query ID: 21, Recall@10: 0.0\n",
      "Query ID: 21, Precision@10: 0.0\n",
      "Query ID: 22, Recall@10: 0.0\n",
      "Query ID: 22, Precision@10: 0.0\n",
      "Query ID: 23, Recall@10: 0.00749063670411985\n",
      "Query ID: 23, Precision@10: 0.2\n",
      "Query ID: 24, Recall@10: 0.0\n",
      "Query ID: 24, Precision@10: 0.0\n",
      "Query ID: 25, Recall@10: 0.00980392156862745\n",
      "Query ID: 25, Precision@10: 0.1\n",
      "Query ID: 26, Recall@10: 0.0\n",
      "Query ID: 26, Precision@10: 0.0\n",
      "Query ID: 27, Recall@10: 0.012875536480686695\n",
      "Query ID: 27, Precision@10: 0.3\n",
      "Query ID: 28, Recall@10: 0.0\n",
      "Query ID: 28, Precision@10: 0.0\n",
      "Query ID: 29, Recall@10: 0.0\n",
      "Query ID: 29, Precision@10: 0.0\n",
      "Query ID: 30, Recall@10: 0.013333333333333334\n",
      "Query ID: 30, Precision@10: 0.2\n",
      "Query ID: 31, Recall@10: 0.01744186046511628\n",
      "Query ID: 31, Precision@10: 0.3\n",
      "Query ID: 32, Recall@10: 0.0\n",
      "Query ID: 32, Precision@10: 0.0\n",
      "Query ID: 33, Recall@10: 0.0\n",
      "Query ID: 33, Precision@10: 0.0\n",
      "Query ID: 34, Recall@10: 0.0\n",
      "Query ID: 34, Precision@10: 0.0\n",
      "Query ID: 35, Recall@10: 0.0\n",
      "Query ID: 35, Precision@10: 0.0\n",
      "Query ID: 36, Recall@10: 0.003236245954692557\n",
      "Query ID: 36, Precision@10: 0.1\n",
      "Query ID: 37, Recall@10: 0.004032258064516129\n",
      "Query ID: 37, Precision@10: 0.1\n",
      "Query ID: 38, Recall@10: 0.012345679012345678\n",
      "Query ID: 38, Precision@10: 0.1\n",
      "Query ID: 39, Recall@10: 0.0\n",
      "Query ID: 39, Precision@10: 0.0\n",
      "Query ID: 40, Recall@10: 0.0\n",
      "Query ID: 40, Precision@10: 0.0\n",
      "Query ID: 41, Recall@10: 0.012987012987012988\n",
      "Query ID: 41, Precision@10: 0.4\n",
      "Query ID: 42, Recall@10: 0.0\n",
      "Query ID: 42, Precision@10: 0.0\n",
      "Query ID: 43, Recall@10: 0.0064516129032258064\n",
      "Query ID: 43, Precision@10: 0.1\n",
      "Query ID: 44, Recall@10: 0.0\n",
      "Query ID: 44, Precision@10: 0.0\n",
      "Query ID: 45, Recall@10: 0.021505376344086023\n",
      "Query ID: 45, Precision@10: 0.4\n",
      "Query ID: 46, Recall@10: 0.018018018018018018\n",
      "Query ID: 46, Precision@10: 0.2\n",
      "Query ID: 47, Recall@10: 0.0\n",
      "Query ID: 47, Precision@10: 0.0\n",
      "Query ID: 48, Recall@10: 0.0\n",
      "Query ID: 48, Precision@10: 0.0\n",
      "Query ID: 49, Recall@10: 0.06493506493506493\n",
      "Query ID: 49, Precision@10: 0.5\n",
      "Query ID: 50, Recall@10: 0.0\n",
      "Query ID: 50, Precision@10: 0.0\n",
      "Query ID: 51, Recall@10: 0.0\n",
      "Query ID: 51, Precision@10: 0.0\n",
      "Query ID: 52, Recall@10: 0.0\n",
      "Query ID: 52, Precision@10: 0.0\n",
      "Query ID: 53, Recall@10: 0.0\n",
      "Query ID: 53, Precision@10: 0.0\n",
      "Query ID: 54, Recall@10: 0.12\n",
      "Query ID: 54, Precision@10: 0.9\n",
      "Query ID: 55, Recall@10: 0.02564102564102564\n",
      "Query ID: 55, Precision@10: 0.1\n",
      "Query ID: 56, Recall@10: 0.03488372093023256\n",
      "Query ID: 56, Precision@10: 0.3\n",
      "Query ID: 57, Recall@10: 0.0\n",
      "Query ID: 57, Precision@10: 0.0\n",
      "Query ID: 58, Recall@10: 0.0\n",
      "Query ID: 58, Precision@10: 0.0\n",
      "Query ID: 59, Recall@10: 0.008547008547008548\n",
      "Query ID: 59, Precision@10: 0.3\n",
      "Query ID: 60, Recall@10: 0.0\n",
      "Query ID: 60, Precision@10: 0.0\n",
      "Query ID: 61, Recall@10: 0.005263157894736842\n",
      "Query ID: 61, Precision@10: 0.1\n",
      "Query ID: 62, Recall@10: 0.0036363636363636364\n",
      "Query ID: 62, Precision@10: 0.1\n",
      "Query ID: 63, Recall@10: 0.0\n",
      "Query ID: 63, Precision@10: 0.0\n",
      "Query ID: 64, Recall@10: 0.019704433497536946\n",
      "Query ID: 64, Precision@10: 0.4\n",
      "Query ID: 65, Recall@10: 0.0\n",
      "Query ID: 65, Precision@10: 0.0\n",
      "Query ID: 66, Recall@10: 0.0\n",
      "Query ID: 66, Precision@10: 0.0\n",
      "Query ID: 67, Recall@10: 0.006369426751592357\n",
      "Query ID: 67, Precision@10: 0.1\n",
      "Query ID: 68, Recall@10: 0.0\n",
      "Query ID: 68, Precision@10: 0.0\n",
      "Query ID: 69, Recall@10: 0.012658227848101266\n",
      "Query ID: 69, Precision@10: 0.1\n",
      "Query ID: 70, Recall@10: 0.008658008658008658\n",
      "Query ID: 70, Precision@10: 0.2\n",
      "Query ID: 71, Recall@10: 0.005434782608695652\n",
      "Query ID: 71, Precision@10: 0.1\n",
      "Query ID: 72, Recall@10: 0.02197802197802198\n",
      "Query ID: 72, Precision@10: 0.2\n",
      "Query ID: 73, Recall@10: 0.0\n",
      "Query ID: 73, Precision@10: 0.0\n",
      "Query ID: 74, Recall@10: 0.0\n",
      "Query ID: 74, Precision@10: 0.0\n",
      "Query ID: 75, Recall@10: 0.017595307917888565\n",
      "Query ID: 75, Precision@10: 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_query(query: str, tfidf_model, tfidf_matrix):\n",
    "    query_tfidf = tfidf_model.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "    ranked_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return ranked_doc_indices, cosine_similarities\n",
    "\n",
    "tfidf_matrix = load_file(\"tfidf_matrix.pickle\")\n",
    "tfidf_model = load_file(\"tfidf_model.pickle\")\n",
    "\n",
    "def getRetrievedQueries(query: str, k=10):\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    ranked_indices, _ = process_query(preprocessed_query, tfidf_model, tfidf_matrix)\n",
    "    idsList = []\n",
    "    for idx in ranked_indices[:k]:\n",
    "        doc_id = list(corpus.keys())[idx]\n",
    "        idsList.append(doc_id)\n",
    "    return idsList\n",
    "\n",
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break  \n",
    "\n",
    "    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "    true_positives = sum(y_true)\n",
    "    recall_at_10 = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"Query ID: {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"Query ID: {query_id}, Precision@10: {precision_at_10}\")    \n",
    "    return recall_at_10\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    calculate_recall_precision(query_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "667e33a0-bd28-4cca-aefe-31a245663f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP@10): 0.3480773074661963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(retrieved_docs) and retrieved_docs[j] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "map_sum = 0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    map_sum += calculate_MAP(query_id)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP@10): {map_sum / len(queries_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b664498-3cad-4fe9-957e-2c91633ba1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
