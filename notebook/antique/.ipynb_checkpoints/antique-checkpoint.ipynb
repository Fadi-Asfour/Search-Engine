{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc6b643e-2755-4f03-b044-ad1dbfa63125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "# Load dataset and corpus\n",
    "import ir_datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dataset = ir_datasets.load(\"antique/train\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('collection.tsv', sep='\\t', header=None, names=['doc_id', 'text'])\n",
    "\n",
    "# Build the corpus dictionary\n",
    "corpus = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    " \n",
    "        if isinstance(row['text'], str):\n",
    "            corpus[row['doc_id']] = row['text']\n",
    "        else: \n",
    "            corpus[row['doc_id']] = \"\"\n",
    "\n",
    "\n",
    "# Convert the corpus to a list of documents and handle NaN values\n",
    "documents = list(corpus.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0e2c4ca-42e2-4de0-a487-17ca2bc860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_country_names(text):\n",
    "\n",
    "    # List of country names and their variations\n",
    "    country_names = {\n",
    "    \"uae\": \"united arab emirates\", \"u.a.e\": \"united arab emirates\",\n",
    "    \"cn\": \"china\", \"china\": \"china\",\n",
    "    \"sy\": \"syria\", \"syria\": \"syria\",\n",
    "    \"usa\": \"united states of america\", \"u.s.a\": \"united states of america\", \n",
    "    \"us\": \"united states of america\", \"u.s.\": \"united states of america\",\n",
    "    \"uk\": \"united kingdom\", \"u.k.\": \"united kingdom\", \"united kingdom\": \"united kingdom\",\n",
    "    \"england\": \"united kingdom\", \"gb\": \"united kingdom\", \"g.b.\": \"united kingdom\",\n",
    "    \"great britain\": \"united kingdom\", \"fr\": \"france\", \"france\": \"france\",\n",
    "    \"de\": \"germany\", \"germany\": \"germany\", \"deutschland\": \"germany\",\n",
    "    \"jp\": \"japan\", \"japan\": \"japan\", \"it\": \"italy\", \"italy\": \"italy\",\n",
    "    \"itália\": \"italy\", \"es\": \"spain\", \"spain\": \"spain\", \"españa\": \"spain\",\n",
    "    \"ru\": \"russia\", \"russia\": \"russia\", \"россия\": \"russia\", \"in\": \"india\",\n",
    "    \"india\": \"india\", \"br\": \"brazil\", \"brazil\": \"brazil\", \"brasil\": \"brazil\",\n",
    "    \"au\": \"australia\", \"australia\": \"australia\", \"ca\": \"canada\", \"canada\": \"canada\",\n",
    "    \"mx\": \"mexico\", \"mexico\": \"mexico\", \"méxico\": \"mexico\", \"za\": \"south africa\",\n",
    "    \"south africa\": \"south africa\", \"southafrica\": \"south africa\", \"kr\": \"south korea\",\n",
    "    \"south korea\": \"south korea\", \"southkorea\": \"south korea\", \"sa\": \"saudi arabia\",\n",
    "    \"saudi arabia\": \"saudi arabia\", \"ksa\": \"saudi arabia\", \"kingdom of saudi arabia\": \"saudi arabia\",\n",
    "    \"tr\": \"turkey\", \"turkey\": \"turkey\", \"trkiye\": \"turkey\", \"ch\": \"switzerland\",\n",
    "    \"switzerland\": \"switzerland\", \"suisse\": \"switzerland\", \"chile\": \"chile\", \n",
    "    \"pt\": \"portugal\", \"portugal\": \"portugal\", \"pl\": \"poland\", \"poland\": \"poland\",\n",
    "    \"polska\": \"poland\", \"eg\": \"egypt\", \"egypt\": \"egypt\", \"egito\": \"egypt\",\n",
    "    \"ng\": \"nigeria\", \"nigeria\": \"nigeria\", \"nigéria\": \"nigeria\", \"ar\": \"argentina\",\n",
    "    \"argentina\": \"argentina\", \"gr\": \"greece\", \"greece\": \"greece\", \"ellada\": \"greece\",\n",
    "    \"se\": \"sweden\", \"sweden\": \"sweden\", \"sverige\": \"sweden\", \"no\": \"norway\",\n",
    "    \"norway\": \"norway\", \"norge\": \"norway\", \"fi\": \"finland\", \"finland\": \"finland\",\n",
    "    \"suomi\": \"finland\", \"nl\": \"netherlands\", \"netherlands\": \"netherlands\", \n",
    "    \"holland\": \"netherlands\", \"vn\": \"vietnam\", \"vietnam\": \"vietnam\", \"hk\": \"hong kong\",\n",
    "    \"hong kong\": \"hong kong\", \"ir\": \"iran\", \"iran\": \"iran\", \"iq\": \"iraq\", \n",
    "    \"iraq\": \"iraq\", \"ph\": \"philippines\", \"philippines\": \"philippines\", \"pk\": \"pakistan\",\n",
    "    \"pakistan\": \"pakistan\", \"th\": \"thailand\", \"thailand\": \"thailand\", \"my\": \"malaysia\",\n",
    "    \"malaysia\": \"malaysia\", \"id\": \"indonesia\", \"indonesia\": \"indonesia\", \n",
    "    \"bd\": \"bangladesh\", \"bangladesh\": \"bangladesh\", \"af\": \"afghanistan\",\n",
    "    \"afghanistan\": \"afghanistan\", \"il\": \"israel\", \"israel\": \"israel\", \"at\": \"austria\",\n",
    "    \"austria\": \"austria\", \"be\": \"belgium\", \"belgium\": \"belgium\", \"cl\": \"chile\",\n",
    "    \"co\": \"colombia\", \"colombia\": \"colombia\", \"cz\": \"czech republic\",\n",
    "    \"czech republic\": \"czech republic\", \"dk\": \"denmark\", \"denmark\": \"denmark\",\n",
    "    \"hu\": \"hungary\", \"hungary\": \"hungary\", \"is\": \"iceland\", \"iceland\": \"iceland\",\n",
    "    \"ie\": \"ireland\", \"ireland\": \"ireland\", \"ke\": \"kenya\", \"kenya\": \"kenya\", \n",
    "    \"lt\": \"lithuania\", \"lithuania\": \"lithuania\", \"lu\": \"luxembourg\", \n",
    "    \"luxembourg\": \"luxembourg\", \"mt\": \"malta\", \"malta\": \"malta\", \"ma\": \"morocco\",\n",
    "    \"morocco\": \"morocco\", \"nz\": \"new zealand\", \"new zealand\": \"new zealand\", \n",
    "    \"pe\": \"peru\", \"peru\": \"peru\", \"ro\": \"romania\", \"romania\": \"romania\", \n",
    "    \"sg\": \"singapore\", \"singapore\": \"singapore\", \"sk\": \"slovakia\", \n",
    "    \"slovakia\": \"slovakia\", \"tw\": \"taiwan\", \"taiwan\": \"taiwan\", \"ua\": \"ukraine\", \n",
    "    \"ukraine\": \"ukraine\", \"ve\": \"venezuela\", \"venezuela\": \"venezuela\"\n",
    "}\n",
    "    return country_names.get(text, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65f7daf9-c8db-4f4e-87bb-8943a9701ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom tokenizer\n",
    "def custom_tokenizer(text: str) -> list[str]:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    tag = tag[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def correct_sentence_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in misspelled:\n",
    "            corrected = spell.correction(token)\n",
    "            if corrected is not None:\n",
    "                tokens[i] = corrected\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes punctuation from tokens.\"\"\"\n",
    "    return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "def remove_apostrophe(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "    return new_tokens\n",
    "def correct_sentence_spelling(tokens):\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        corrected_token = str(TextBlob(token).correct())\n",
    "        corrected_tokens.append(corrected_token)\n",
    "    return corrected_tokens\n",
    "def remove_markers(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "    return new_tokens\n",
    "def remove_links(text):\n",
    "    url_pattern = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
    "    cleaned_text = re.sub(url_pattern, \"\", text)\n",
    "    \n",
    "    return cleaned_text  \n",
    "    \n",
    "\n",
    "def remove_apostrophe(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Removes apostrophes from tokens.\"\"\"\n",
    "    return [token.replace(\"'\", \" \") for token in tokens]\n",
    "\n",
    "def replace_under_score_with_space(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Replaces underscores with spaces in tokens.\"\"\"\n",
    "    return [re.sub(r'_', ' ', token) for token in tokens]\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocesses the input text by tokenizing, removing punctuation, stopwords, and then stemming and lemmatizing.\"\"\"\n",
    "    #remove links\n",
    "    text=remove_links(text)\n",
    "        \n",
    "    # Convert text to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Correct spelling\n",
    "    words = correct_sentence_spelling(words)\n",
    "    \n",
    "    # Further token cleaning\n",
    "    words = remove_markers(words)\n",
    "    words = replace_under_score_with_space(words)\n",
    "    words = remove_apostrophe(words)\n",
    "    words = [normalize_country_names(word) for word in words]\n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb86ef-8ccf-4031-ad88-ec0eaf02a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer setup\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, preprocessor=preprocess_text)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "tfidf_model = vectorizer\n",
    "print(\"TF-IDF DataFrame created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24361a89-a132-44ba-a65d-d4a655dfa904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save and load functions for TF-IDF data\n",
    "def save_file(file_location: str, content):\n",
    "    if os.path.exists(file_location):\n",
    "        os.remove(file_location)\n",
    "    with open(file_location, 'wb') as handle:\n",
    "        pickle.dump(content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_file(file_location: str):\n",
    "    with open(file_location, 'rb') as handle:\n",
    "        content = pickle.load(handle)\n",
    "    return content\n",
    "\n",
    "def save_tfidf_data(tfidf_matrix, tfidf_model):\n",
    "    save_file(\"tfidf_matrix.pickle\", tfidf_matrix)\n",
    "    save_file(\"tfidf_model.pickle\", tfidf_model)\n",
    "\n",
    "\n",
    "save_tfidf_data(tfidf_matrix, tfidf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e33a0-bd28-4cca-aefe-31a245663f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str, tfidf_model, tfidf_matrix):\n",
    "    query_tfidf = tfidf_model.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "    ranked_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return ranked_doc_indices, cosine_similarities\n",
    "\n",
    "tfidf_matrix = load_file(\"tfidf_matrix.pickle\")\n",
    "tfidf_model = load_file(\"tfidf_model.pickle\")\n",
    "\n",
    "def getRetrievedQueries(query: str, k=10):\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    ranked_indices, _ = process_query(preprocessed_query, tfidf_model, tfidf_matrix)\n",
    "    idsList = []\n",
    "    for idx in ranked_indices[:k]:\n",
    "        doc_id = list(corpus.keys())[idx]\n",
    "        idsList.append(doc_id)\n",
    "    return idsList\n",
    "\n",
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break  \n",
    "\n",
    "    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "    true_positives = sum(y_true)\n",
    "    recall_at_10 = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"Query ID: {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"Query ID: {query_id}, Precision@10: {precision_at_10}\")    \n",
    "    return recall_at_10\n",
    "queries_ids = {}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    queries_ids.update({qrel[0]: ''})\n",
    "    \n",
    "for query_id in list(queries_ids.keys()):\n",
    "    calculate_recall_precision(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b664498-3cad-4fe9-957e-2c91633ba1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs = []\n",
    "    for qrel in dataset.qrels_iter():\n",
    "        if qrel[0] == query_id and qrel[2] > 0:\n",
    "            relevant_docs.append(qrel[1])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for query in dataset.queries_iter():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs = getRetrievedQueries(query[1])\n",
    "            break\n",
    "\n",
    "    pk_sum = 0\n",
    "    total_relevant = 0\n",
    "    for i in range(1, 11):\n",
    "        relevant_ret = 0\n",
    "        for j in range(i):\n",
    "            if j < len(retrieved_docs) and retrieved_docs[j] in relevant_docs:\n",
    "                relevant_ret += 1\n",
    "        p_at_k = (relevant_ret / i) * (1 if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs else 0)\n",
    "        pk_sum += p_at_k\n",
    "        if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs:\n",
    "            total_relevant += 1\n",
    "\n",
    "    return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "queries_ids = {qrel[0]: '' for qrel in dataset.qrels_iter()}\n",
    "\n",
    "map_sum = 0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    map_sum += calculate_MAP(query_id)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP@10): {map_sum / len(queries_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931efdd-56a8-4583-99fb-ec10f368d661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395feac-eabd-4eca-a871-259502898a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
