{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Setup and Imports \n",
    "This section initializes the environment, sets up necessary constants, and imports required libraries and modules"
   ],
   "id": "3eba801c9ca6dee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T09:37:32.368814Z",
     "start_time": "2024-06-05T09:37:27.681293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import ndarray\n",
    "\n",
    "from service.text_preprocessing.text_preprocessor import TextPreprocessing\n",
    "from user_vector.user_vector import VectorDBHelper\n",
    "from utils_functions.string_manager import antique_embedding_vector_db_path, antique_embedding_model_path\n",
    "import ir_datasets"
   ],
   "id": "2545a39ff29f49e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Define Matcher Class",
   "id": "8d8bcd3cb8589d82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T09:37:32.383822Z",
     "start_time": "2024-06-05T09:37:32.368814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AntiqueEmbMatcher:\n",
    "\n",
    "    def __init__(self, text_processor: TextPreprocessing, n_result: int = 5000):\n",
    "        vectors_storage_path: str = antique_embedding_vector_db_path\n",
    "        self.vector_db_instance = VectorDBHelper.get_instance(db_path=vectors_storage_path)\n",
    "        self.text_processor = text_processor\n",
    "        self.model_name = 'antique'\n",
    "        self.n_result = n_result\n",
    "\n",
    "        model_storage_path: str = antique_embedding_model_path\n",
    "        self.model: Word2Vec = Word2Vec.load(model_storage_path)\n",
    "\n",
    "        self.vector_size = self.model.vector_size\n",
    "        # self.n_results = 0\n",
    "\n",
    "    def match(self, text: str):\n",
    "\n",
    "        processed_query: List[str] = self.text_processor.process_text(text)\n",
    "        query_embeddings: List = self.__vectorize_query(processed_query).tolist()\n",
    "\n",
    "        results = self.vector_db_instance.query_db(\n",
    "            self.model_name,\n",
    "            query_embeddings,\n",
    "            n_results=self.n_result\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __vectorize_query(self, query_words: list[str]) -> ndarray:\n",
    "\n",
    "        query_vectors = [self.model.wv[word] for word in query_words if word in self.model.wv]\n",
    "\n",
    "        if query_vectors:\n",
    "            query_vec = np.mean(query_vectors, axis=0)\n",
    "        else:\n",
    "            query_vec = np.zeros(self.vector_size)\n",
    "\n",
    "        return query_vec"
   ],
   "id": "40b29214e501f11e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Define Evaluator class",
   "id": "3ad1d1fa9f724b0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T09:37:32.416973Z",
     "start_time": "2024-06-05T09:37:32.383822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AntiqueEvaluator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.antique_matcher = AntiqueEmbMatcher(text_processor=TextPreprocessing())\n",
    "        self.antique_dataset = ir_datasets.load('antique/train')\n",
    "\n",
    "    def compute_relevance_scores(self, query_text: str) -> List[str]:\n",
    "        result = self.antique_matcher.match(query_text)\n",
    "        relevance_scores = [item['id'] for item in result]\n",
    "        return relevance_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_precision_recall_at_k(self, relevant_docs, retrieved_docs, k):\n",
    "        y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs[:k]]\n",
    "        true_positives = sum([1 for i in range(len(y_true)) if y_true[i] == 1])\n",
    "        recall_at_k = true_positives / len(relevant_docs)\n",
    "        precision_at_k = true_positives / k\n",
    "        print(f\"Recall@{k}: {recall_at_k}\")\n",
    "        print(f\"Precision@{k}: {precision_at_k}\")\n",
    "        return precision_at_k, recall_at_k\n",
    "\n",
    "    def evaluate_map(self):\n",
    "        queries_ids = {qrel[0]: '' for qrel in self.antique_dataset.qrels_iter()}\n",
    "\n",
    "        map_sum = 0\n",
    "        for query_id in list(queries_ids.keys()):\n",
    "            map_sum += self.calculate_MAP(query_id)\n",
    "        return map_sum / len(queries_ids)\n",
    "\n",
    "    def evaluate_mrr(self):\n",
    "        queries_ids = {}\n",
    "        for qrel in self.antique_dataset.qrels_iter():\n",
    "            queries_ids.update({qrel.query_id: ''})\n",
    "\n",
    "        mrr_sum = 0\n",
    "        for query_id in list(queries_ids.keys()):\n",
    "            mrr_sum += self.calculate_MRR(query_id)\n",
    "\n",
    "        return mrr_sum / len(queries_ids)\n",
    "\n",
    "    def calculate_MAP(self, query_id):\n",
    "        relevant_docs = []\n",
    "        retrieved_docs = []\n",
    "\n",
    "        # Get relevant documents for the query\n",
    "        for qrel in self.antique_dataset.qrels_iter():\n",
    "            if qrel.query_id == query_id and qrel.relevance > 0:\n",
    "                relevant_docs.append(qrel.doc_id)\n",
    "\n",
    "        # Get retrieved documents for the query\n",
    "        for query in self.antique_dataset.queries_iter():\n",
    "            if query.query_id == query_id:\n",
    "                retrieved_docs = self.compute_relevance_scores(query.text)\n",
    "                break\n",
    "\n",
    "        # Compute mean average precision\n",
    "        pk_sum = 0\n",
    "        total_relevant = 0\n",
    "        for i in range(1, 11):\n",
    "            relevant_ret = 0\n",
    "            for j in range(i):\n",
    "                if j < len(retrieved_docs) and retrieved_docs[j] in relevant_docs:\n",
    "                    relevant_ret += 1\n",
    "            p_at_k = (relevant_ret / i) * (\n",
    "                1 if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs else 0)\n",
    "            pk_sum += p_at_k\n",
    "            if i - 1 < len(retrieved_docs) and retrieved_docs[i - 1] in relevant_docs:\n",
    "                total_relevant += 1\n",
    "\n",
    "        return 0 if total_relevant == 0 else pk_sum / total_relevant\n",
    "\n",
    "    def calculate_MRR(self, query_id):\n",
    "        relevant_docs = []\n",
    "        for qrel in self.antique_dataset.qrels_iter():\n",
    "            if qrel.query_id == query_id and qrel.relevance > 0:\n",
    "                relevant_docs.append(qrel.doc_id)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        for query in self.antique_dataset.queries_iter():\n",
    "            if query.query_id == query_id:\n",
    "                retrieved_docs = self.compute_relevance_scores(query.text)\n",
    "                break\n",
    "\n",
    "        for i, result in enumerate(retrieved_docs):\n",
    "            if result in relevant_docs:\n",
    "                return 1 / (i + 1)\n",
    "\n",
    "        return 0"
   ],
   "id": "b7a34a3995e7a9d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Perform Evaluation",
   "id": "5017331be5a84f5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T09:37:38.367482Z",
     "start_time": "2024-06-05T09:37:32.420989Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator = AntiqueEvaluator()",
   "id": "3c33803f71c06054",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T09:38:04.184438Z",
     "start_time": "2024-06-05T09:37:38.367482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "map_value = evaluator.evaluate_map()\n",
    "mrr_value = evaluator.evaluate_mrr()\n",
    "print(f\"Mean Average Precision (MAP) : {map_value}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR) : {mrr_value}\")"
   ],
   "id": "699a1fbcb9f3a911",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m map_value \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate_map()\n\u001B[0;32m      2\u001B[0m mrr_value \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate_mrr()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean Average Precision (MAP) : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmap_value\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 27\u001B[0m, in \u001B[0;36mAntiqueEvaluator.evaluate_map\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     25\u001B[0m map_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(queries_ids\u001B[38;5;241m.\u001B[39mkeys()):\n\u001B[1;32m---> 27\u001B[0m     map_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_MAP(query_id)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m map_sum \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(queries_ids)\n",
      "Cell \u001B[1;32mIn[3], line 53\u001B[0m, in \u001B[0;36mAntiqueEvaluator.calculate_MAP\u001B[1;34m(self, query_id)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantique_dataset\u001B[38;5;241m.\u001B[39mqueries_iter():\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query\u001B[38;5;241m.\u001B[39mquery_id \u001B[38;5;241m==\u001B[39m query_id:\n\u001B[1;32m---> 53\u001B[0m         retrieved_docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_relevance_scores(query\u001B[38;5;241m.\u001B[39mtext)\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Compute mean average precision\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 9\u001B[0m, in \u001B[0;36mAntiqueEvaluator.compute_relevance_scores\u001B[1;34m(self, query_text)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_relevance_scores\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m      8\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantique_matcher\u001B[38;5;241m.\u001B[39mmatch(query_text)\n\u001B[1;32m----> 9\u001B[0m     relevance_scores \u001B[38;5;241m=\u001B[39m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m result]\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m relevance_scores\n",
      "Cell \u001B[1;32mIn[3], line 9\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_relevance_scores\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m      8\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantique_matcher\u001B[38;5;241m.\u001B[39mmatch(query_text)\n\u001B[1;32m----> 9\u001B[0m     relevance_scores \u001B[38;5;241m=\u001B[39m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m result]\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m relevance_scores\n",
      "\u001B[1;31mKeyError\u001B[0m: 'id'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c8659e91b017c14",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
